{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rhedsantiago/Gradient_Descent/blob/main/Assignment1Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "# Class to implement the gradient function at the end\n",
        "class GradientFunction():\n",
        "\n",
        "  # Constructor - takes in the dataframe of either training or testing data\n",
        "  # If you are taking in testing data, you only have to use normalize_variables and gradient_descent function\n",
        "\n",
        "  def __init__(self, df):\n",
        "    self.cars = df\n",
        "    self.bias = 0.0\n",
        "    self.weights = []\n",
        "    self.X_vars = pd.DataFrame\n",
        "    self.Y = pd.DataFrame\n",
        "    self.Y_hat = np.array([])\n",
        "    self.updated_weights = []\n",
        "    self.log = {}\n",
        "\n",
        "  # normalize our variables\n",
        "  def normalize_variables(self):\n",
        "    self.X_vars= self.cars[['cylinders', 'displacement', 'horsepower', 'model_year']]\n",
        "    self.Y = self.cars['mpg']\n",
        "    self.Y = np.array((self.Y - self.Y.mean())/self.Y.std())                        #<-----Normalizing the data\n",
        "    self.X_vars = self.X_vars.apply(lambda rec:(rec-rec.mean())/rec.std(), axis=0)\n",
        "\n",
        "  # start off with any weights, choose randomly\n",
        "  def choose_random_weights(self, randomizer):\n",
        "    self.bias = random.random()\n",
        "    self.weights = np.random.rand(randomizer)\n",
        "\n",
        "  # get the estimated y function based off of the random weights from earlier\n",
        "  def predict_y_func(self):\n",
        "    self.Y_hat = self.bias + np.dot(self.X_vars, self.weights)\n",
        "\n",
        "  # calculate the cost error function, used for testing. We want this as close to 0 as possible\n",
        "  def calculate_cost(self):\n",
        "    Y_residual = self.Y - self.Y_hat #actual - predicted\n",
        "    self.predict_y_func()\n",
        "    return np.sum(np.dot(Y_residual.T, Y_residual))/len(self.Y-Y_residual)\n",
        "\n",
        "  # function to update the weights from the previously randomly selected ones\n",
        "  def update_weights(self, learning_rate):\n",
        "    count = 0\n",
        "\n",
        "    # keep performing this algorithm until our cost function is less than 0.5\n",
        "    # this way, we get the cost function as close to 0 as possible\n",
        "\n",
        "    log_file = open(\"weights_tested_results.txt\", \"w\")\n",
        "\n",
        "    while (self.calculate_cost() > 0.5):\n",
        "      self.predict_y_func()\n",
        "      self.bias=self.bias-learning_rate*(np.sum(self.Y_hat-self.Y)*2)/len(self.Y)\n",
        "      self.weights=self.weights-learning_rate*(np.dot((self.Y_hat-self.Y),self.X_vars)*2)/len(self.Y)\n",
        "      self.calculate_cost()\n",
        "      count += 1\n",
        "      key = 'trial ' + str(count)\n",
        "      tracker = []\n",
        "      tracker.append(self.bias)\n",
        "      tracker.append(self.weights[0])\n",
        "      tracker.append(self.weights[1])\n",
        "      tracker.append(self.weights[2])\n",
        "      tracker.append(self.weights[3])\n",
        "      self.log[key] = tracker\n",
        "\n",
        "\n",
        "      log_file.write(\"Trial #: \" )\n",
        "      log_file.write(str(key) + \"\\n\")\n",
        "      log_file.write(\"Weights: \")\n",
        "      log_file.write(str(tracker) + \"\\n\")\n",
        "      log_file.write('------------------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "  # combine the bias and weights into one array\n",
        "  def adjust_values(self):\n",
        "    self.updated_weights.append(self.bias)\n",
        "    self.updated_weights.append(self.weights[0])\n",
        "    self.updated_weights.append(self.weights[1])\n",
        "    self.updated_weights.append(self.weights[2])\n",
        "    self.updated_weights.append(self.weights[3])\n",
        "\n",
        "  # Function using residual that will be plugged into the gradient_descent function\n",
        "  def my_gradient(self):\n",
        "      res = self.updated_weights[0] + (np.dot(self.updated_weights[1],self.X_vars['cylinders']) - self.Y) + (np.dot(self.updated_weights[2],self.X_vars['displacement']) - self.Y ) + (np.dot(self.updated_weights[3],self.X_vars['horsepower']) - self.Y) + (np.dot(self.updated_weights[4],self.X_vars['model_year']) - self.Y)\n",
        "      #return res.mean(), (np.dot(res, x)).mean()  # .mean() is a method of np.ndarray\n",
        "      return res.mean(), (np.dot(res, self.X_vars['cylinders'])).mean(), (np.dot(res, self.X_vars['displacement'])).mean(), (np.dot(res, self.X_vars['horsepower'])).mean(), (np.dot(res, self.X_vars['model_year'])).mean()\n",
        "      # for vector of x (res * x[0]).mean(), res (res*x[1]).mean()\n",
        "\n",
        "  # Gradient descent function from class\n",
        "  def gradient_descent(self, learn_rate=0.1, n_iter=50, tolerance=1e-06):\n",
        "    vector = self.updated_weights\n",
        "    for _ in range(n_iter):\n",
        "      diff = -learn_rate * np.array(self.my_gradient())\n",
        "      if np.all(np.abs(diff) <= tolerance):\n",
        "        break\n",
        "      vector += diff\n",
        "    return vector\n",
        "\n",
        "cars = pd.read_csv('https://raw.githubusercontent.com/Jerpac/CS4375/main/auto-mpg.data', delim_whitespace = True)\n",
        "\n",
        "# Pre-processing the data ----------------------\n",
        "# this entire thing will be separate from the class?\n",
        "\n",
        "cars.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']\n",
        "\n",
        "cars = cars.dropna()\n",
        "cars = cars.drop_duplicates()\n",
        "cars = cars.drop(columns = ['car_name', 'origin', 'weight', 'acceleration'])\n",
        "cars = cars.drop(cars[cars['horsepower'] == '?'].index)\n",
        "\n",
        "cars[\"cylinders\"] = pd.to_numeric(cars[\"cylinders\"], downcast=\"integer\")\n",
        "cars[\"displacement\"] = pd.to_numeric(cars[\"displacement\"], downcast=\"float\")\n",
        "cars[\"horsepower\"] = pd.to_numeric(cars[\"horsepower\"], downcast=\"float\")\n",
        "cars[\"model_year\"] = pd.to_numeric(cars[\"model_year\"], downcast=\"integer\")\n",
        "\n",
        "cars = cars.reset_index(drop=True)\n",
        "\n",
        "training_data = cars.sample(frac = 0.8, random_state = 100)\n",
        "test_data = cars.drop(training_data.index)\n",
        "# Create the class\n",
        "car_gradient = GradientFunction(training_data)\n",
        "# Normalize our variables\n",
        "car_gradient.normalize_variables()\n",
        "# get random values for bias and weights\n",
        "car_gradient.choose_random_weights(4)\n",
        "# Calculate the output of Y\n",
        "car_gradient.predict_y_func()\n",
        "# Calculate the cost function (just to see where we are)\n",
        "car_gradient.calculate_cost()\n",
        "# Update the weights for our cost function\n",
        "car_gradient.update_weights(0.01)\n",
        "# Adjust the values\n",
        "car_gradient.adjust_values()\n",
        "# Get my gradient\n",
        "car_gradient.my_gradient()\n",
        "# Call the gradient descent algorithm\n",
        "print(f'Result of Gradient Descent Function Using Training Data: {car_gradient.gradient_descent(0.0001, 1)}')\n",
        "\n",
        "weights = car_gradient.updated_weights\n",
        "car_gradient2 = GradientFunction(test_data)\n",
        "car_gradient2.updated_weights = weights\n",
        "car_gradient2.normalize_variables()\n",
        "\n",
        "print(f'Result of Gradient Descent Function Using Test Data: {car_gradient2.gradient_descent(0.0001, 1)}')\n",
        "print(f'Weights used for both: {weights}')"
      ],
      "metadata": {
        "id": "-TDhB_XLs3zI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b972df20-07aa-403d-d6e5-c3d84e1bd531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of Gradient Descent Function Using Training Data: [ 0.19920926 -0.18274627 -0.39671266  0.0198157   0.83586108]\n",
            "Result of Gradient Descent Function Using Test Data: [ 0.19920926 -0.1244862  -0.33590638  0.07625672  0.79950752]\n",
            "Weights used for both: [0.19922918226731534, -0.1046499718165445, -0.31571513515044647, 0.09682568170532603, 0.7900747096872023]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UaovGNFddzZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
